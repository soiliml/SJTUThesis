%# -*- coding: utf-8-unix -*-
%%==================================================
%% abstract.tex for SJTU Master Thesis
%%==================================================

\begin{abstract}

最近，长短时间变化神经网络语言模型（LSTM LM）因为它的记忆单元出色的对历史信息的记忆能力，得到了语言和语音学者广泛的关注。
对语言模型进行一系列的结构优化可以使其在自动语音识别任务中表现得更好。
本文首先针对语言模型的结构化特性进行学习率自适应的研究，
获得最好的调整方法，成功使得语言模型的不同结构能自动调整学习率，为后面研究工作打好基础。
接着，由于将辅助信息(如上下文特性)加入到LSTM LM中，能在语言模型的混淆度(PPL)中体现出比较大的语言模型的性能提升。
然而，在大量的词汇连续语音识别(LVCSR)任务中，辅助信息的不恰当的输入不会使单词错误率(WER)获得同样的增益。
为了解决这一问题，本文提出了一种多视角融合的LSTM语言模型结构。首先建立了一种在线单向lstm作为一种标记模型，可以生成词同步的辅助信息特征。
然后将标记模型的辅助特征与单词序列结合起来，训练多视角单向LSTM语言模型。研究并比较了不同的标记模型和语言模型的训练模式。
在英文数据集PTB、Fisher和中文短信S数据集上对新架构进行了评价，结果表明，新的模型结构不仅能够提升混淆度，也能减小语音识别任务中的词错误率和句子错误率。
除此之外，本文将教师-学生模型引入到语言模型中，
探究多种模型之间的教授方式，并最终能大幅度提升小语言模型的性能。


\keywords{\large 关键词： \quad 自动语音识别 \quad 语言模型 \quad 神经网络 \quad 多视角语言模型 \quad 学习率自适应 \quad 教师-学生模型}
\end{abstract}

\begin{englishabstract}
Recently long short-term memory language model (LSTM LM) has received tremendous interests from both language and speech communities, 
due to its superiorty on modelling long-term dependency. 
A series of structural optimizations to the language model can make it perform better in auto speech recognition tasks.
In this paper, we firstly concern learning rate adaption task aiming at the structural characteristics of the language model.
The best adjustment method is obtained, which enables the different structure of language model to automatically adjust the learning rate and lay a foundation for the research work in the future.
Later, because integrating auxiliary information, 
such as context feature, into the LSTM LM has shown improved performance in perplexity (PPL). 
However, improper feed of auxiliary information won't give consistent gain on word error rate (WER) in a large vocabulary continuous speech recognition (LVCSR) task. 
To solve this problem, a multi-view LSTM LM architecture combining a tagging model is proposed in this paper.
 Firstly an on-line unidirectional LSTM-RNN is built as a tagging model, which can generate word-synchronized auxiliary feature. 
Then the auxiliary feature from the tagging model is combined with the word sequence to train a multi-view unidirectional LSTM LM. 
Different training modes for the tagging model and language model are explored and compared. The new architecture is evaluated on PTB, 
Fisher English and SMS Chinese data sets, and the results show that not only LM PPL promotion is observed, but also the improvements can be well transferred to WER reduction in ASR-rescore task.
In addition, this paper introduces the teacher - student model into the language model.
Explore the way of teaching among various models and ultimately improve the performance of small language models.
\englishkeywords{\large ASR, language model, neural network, multi-view, learning rate adaptation, teacher-student model}
\end{englishabstract}

