%# -*- coding: utf-8-unix -*-
%%==================================================
%% abstract.tex for SJTU Master Thesis
%%==================================================

\begin{abstract}

最近，自动语音识别（ASR）和自然语言处理（NLP）的研究推动了语言模型的发展。其中，循环神经网络语言模型（RNNLM）由于其存在特殊的环状结构，使得训练过程中的历史信息可以被记录并作用于当前单词的训练，而比N-gram等传统语言模型表现更佳。近几年，Mikolov等人证明在训练语言模型时加入额外信息同原始数据一起训练能有效提高语言模型的性能，然而引入的信息过于单调，也没有针对中文做研究。因此本文从多视角融合的角度针对中文去考虑。

因为语言本身具有一些约束特性，如单词的识别会依赖于词性的连接规律、话题的约束和场景地点的区分等。本文首先将数据集预处理，分别通过对应的算法得到数据对应的词性、话题和场景等不同的信息，并进行标注得到新的数据集。然后，以词为单位对带标记的数据进行解析，得到其词向量和对应信息的特征向量。最后将这些向量一同作为RNNLM的输入层进行训练。通过多组对比实验可以发现，多视角融合的RNNLM相比于传统的RNNLM在各类数据上均有8%左右的perplexity性能提升，在ASR的实验中也表现出了显著提高。

在这篇论文中，我们将首先介绍语言模型的基本概念和传统的语言模型；然后介绍RNNLM的结构和训练流程，以及其关键数学理论基础；接着会详细说明多视角融合的RNNLM视角提取量化，以及训练框架和原理；最后展示和讨论实验结果。

\keywords{\large 关键词： \quad 自动语音识别 \quad 语言模型 \quad 神经网络 \quad 多视角语言模型}
\end{abstract}

\begin{englishabstract}
Recently， research in automatic speech recognition （ASR） and natural language processing （NLP） promotes the development of language model. Among them， the RNN （recurrent neural network） language model which has special ring structure can record the history of training and use it in current training， thus performing better than traditional language model， for instance， N-gram language model. Mikolov demonstrates that the performance of language model can be effectively improved by combining the original data with additional information. However the information they used is quite simple，and also don’t aim at Chinese. our work will focus on an advanced method which is combine different views and aimed at Chinese.

The constraints features of the language can help the training process of language model in neural network judge better. For example， word recognition depends on the connection rule of POS（part-of-speech）， the constraints of topic，the scene distinction and location. First we preprocess the data set and obtain the information about POS， constraints of topic and scene distinction by using the corresponding algorithm. And we present a new data set by marking the information. Then parsing the marked data measured in word and obtain word vector and feature vector of corresponding information. After that we take these vectors as the input layer of RNN language model for training. By comparing multiple sets of experiments， multi-view RNN language model has 8% of improvement in perplexity performance on all kinds of data compared with the traditional RNN language model. The experiment of ASR also indicates significant improvement.

In this paper， we introduce the basic concept of language model and traditional  language model， explain the structure of the RNN language model， expand the training process and the mathematical theory. Then we illustrate the process of view extraction，  quantification and training framework with the principle in multi-view RNN language model. Finally we present and discuss the results of experiment for futher exploration.

\englishkeywords{\large SJTU, master thesis, XeTeX/LaTeX template}
\end{englishabstract}

